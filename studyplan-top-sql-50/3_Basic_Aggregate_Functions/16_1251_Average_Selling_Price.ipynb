{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [1251. Average Selling Price](https://leetcode.com/problems/average-selling-price/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d53c4ffefc39db36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Prices\n",
    "\n",
    "<pre>+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| start_date    | date    |\n",
    "| end_date      | date    |\n",
    "| price         | int     |\n",
    "+---------------+---------+</pre>\n",
    "(product_id, start_date, end_date) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table indicates the price of the product_id in the period from start_date to end_date.\n",
    "For each product_id there will be no two overlapping periods. That means there will be no two intersecting periods for the same product_id.\n",
    " \n",
    "\n",
    "Table: UnitsSold\n",
    "\n",
    "<pre>+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| purchase_date | date    |\n",
    "| units         | int     |\n",
    "+---------------+---------+</pre>\n",
    "This table may contain duplicate rows.\n",
    "Each row of this table indicates the date, units, and product_id of each product sold. \n",
    " \n",
    "\n",
    "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Prices table:\n",
    "<pre>+------------+------------+------------+--------+\n",
    "| product_id | start_date | end_date   | price  |\n",
    "+------------+------------+------------+--------+\n",
    "| 1          | 2019-02-17 | 2019-02-28 | 5      |\n",
    "| 1          | 2019-03-01 | 2019-03-22 | 20     |\n",
    "| 2          | 2019-02-01 | 2019-02-20 | 15     |\n",
    "| 2          | 2019-02-21 | 2019-03-31 | 30     |\n",
    "+------------+------------+------------+--------+</pre>\n",
    "UnitsSold table:\n",
    "<pre>+------------+---------------+-------+\n",
    "| product_id | purchase_date | units |\n",
    "+------------+---------------+-------+\n",
    "| 1          | 2019-02-25    | 100   |\n",
    "| 1          | 2019-03-01    | 15    |\n",
    "| 2          | 2019-02-10    | 200   |\n",
    "| 2          | 2019-03-22    | 30    |\n",
    "+------------+---------------+-------+</pre>\n",
    "Output: \n",
    "<pre>+------------+---------------+\n",
    "| product_id | average_price |\n",
    "+------------+---------------+\n",
    "| 1          | 6.96          |\n",
    "| 2          | 16.96         |\n",
    "+------------+---------------+</pre>\n",
    "Explanation: \n",
    "Average selling price = Total Price of Product / Number of products sold.\n",
    "Average selling price for product 1 = ((100 * 5) + (15 * 20)) / 115 = 6.96\n",
    "Average selling price for product 2 = ((200 * 15) + (30 * 30)) / 230 = 16.96"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4edaf274919baad0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1, '2019-02-17', '2019-02-28', 5], [1, '2019-03-01', '2019-03-22', 20], [2, '2019-02-01', '2019-02-20', 15],\n",
    "        [2, '2019-02-21', '2019-03-31', 30]]\n",
    "prices = pd.DataFrame(data, columns=['product_id', 'start_date', 'end_date', 'price']).astype(\n",
    "    {'product_id': 'Int64', 'start_date': 'datetime64[ns]', 'end_date': 'datetime64[ns]', 'price': 'Int64'})\n",
    "data = [[1, '2019-02-25', 100], [1, '2019-03-01', 15], [2, '2019-02-10', 200], [2, '2019-03-22', 30]]\n",
    "units_sold = pd.DataFrame(data, columns=['product_id', 'purchase_date', 'units']).astype(\n",
    "    {'product_id': 'Int64', 'purchase_date': 'datetime64[ns]', 'units': 'Int64'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:38:00.211382Z",
     "start_time": "2023-11-07T13:37:59.114282100Z"
    }
   },
   "id": "929f6f9e335f5999"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/06 18:23:36 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/06 18:23:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/06 18:23:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----+\n",
      "|product_id|start_date         |end_date           |price|\n",
      "+----------+-------------------+-------------------+-----+\n",
      "|1         |2019-02-17 00:00:00|2019-02-28 00:00:00|5    |\n",
      "|1         |2019-03-01 00:00:00|2019-03-22 00:00:00|20   |\n",
      "|2         |2019-02-01 00:00:00|2019-02-20 00:00:00|15   |\n",
      "|2         |2019-02-21 00:00:00|2019-03-31 00:00:00|30   |\n",
      "+----------+-------------------+-------------------+-----+\n"
     ]
    }
   ],
   "source": [
    "#to pyspark df\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "prices_df = spark.createDataFrame(prices)\n",
    "prices_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:38:18.726566700Z",
     "start_time": "2023-11-07T13:38:00.220194100Z"
    }
   },
   "id": "7dd5371f7a18b38"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----+\n",
      "|product_id|purchase_date      |units|\n",
      "+----------+-------------------+-----+\n",
      "|1         |2019-02-25 00:00:00|100  |\n",
      "|1         |2019-03-01 00:00:00|15   |\n",
      "|2         |2019-02-10 00:00:00|200  |\n",
      "|2         |2019-03-22 00:00:00|30   |\n",
      "+----------+-------------------+-----+\n"
     ]
    }
   ],
   "source": [
    "units_sold_df = spark.createDataFrame(units_sold)\n",
    "units_sold_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:38:19.916896400Z",
     "start_time": "2023-11-07T13:38:18.722289300Z"
    }
   },
   "id": "93fdfabab91aaa9b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=========================================================(16 + 0) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|average_price|\n",
      "+----------+-------------+\n",
      "|         1|         6.96|\n",
      "|         2|        16.96|\n",
      "+----------+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# solving in spark dataframe\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "prices_df.alias('a').join(units_sold_df,\n",
    "                          on=(prices_df['product_id'] == units_sold_df['product_id'])\n",
    "                             & ((prices_df['start_date'] <= units_sold_df['purchase_date'])\n",
    "                                & (prices_df['end_date'] >= units_sold_df['purchase_date'])),\n",
    "                          how='inner') \\\n",
    "    .groupBy('a.product_id')\\\n",
    "    .agg(F.round(F.sum(F.col('price') * F.col('units')) / F.sum('units'), 2).alias('average_price'))\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:38:23.458070200Z",
     "start_time": "2023-11-07T13:38:19.911185800Z"
    }
   },
   "id": "7e8ff560be771c4d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=======>                                                (2 + 14) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|average_price|\n",
      "+----------+-------------+\n",
      "|         1|         6.96|\n",
      "|         2|        16.96|\n",
      "+----------+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# solving in Spark SQL\n",
    "\n",
    "prices_df.createOrReplaceTempView('prices')\n",
    "units_sold_df.createOrReplaceTempView('unitsSold')\n",
    "\n",
    "spark.sql('''\n",
    "select prices.product_id, round(sum(price*units)/sum(units),2) as average_price \n",
    "from prices\n",
    "inner join unitssold\n",
    "on prices.product_id = unitssold.product_id\n",
    "and (prices.start_date<=unitssold.purchase_date and prices.end_date>=unitssold.purchase_date)\n",
    "group by prices.product_id;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:38:25.116845600Z",
     "start_time": "2023-11-07T13:38:23.448297100Z"
    }
   },
   "id": "5722d8426d8eb86c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:38:26.065621Z",
     "start_time": "2023-11-07T13:38:25.107128Z"
    }
   },
   "id": "5c9eb474d9e86069"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
