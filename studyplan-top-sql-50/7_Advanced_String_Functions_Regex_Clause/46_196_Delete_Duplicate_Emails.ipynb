{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [196. Delete Duplicate Emails](https://leetcode.com/problems/delete-duplicate-emails/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67a8ab42018519d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Person\n",
    "\n",
    "<pre>+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| email       | varchar |\n",
    "+-------------+---------+</pre>\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains an email. The emails will not contain uppercase letters.\n",
    " \n",
    "\n",
    "Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id.\n",
    "\n",
    "For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.\n",
    "\n",
    "For Pandas users, please note that you are supposed to modify Person in place.\n",
    "\n",
    "After running your script, the answer shown is the Person table. The driver will first compile and run your piece of code and then show the Person table. The final order of the Person table does not matter.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Person table:\n",
    "<pre>+----+------------------+\n",
    "| id | email            |\n",
    "+----+------------------+\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "| 3  | john@example.com |\n",
    "+----+------------------+</pre>\n",
    "Output: \n",
    "<pre>+----+------------------+\n",
    "| id | email            |\n",
    "+----+------------------+\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "+----+------------------+</pre>\n",
    "Explanation: john@example.com is repeated two times. We keep the row with the smallest Id = 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9bc2928e9bbe697"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1, 'john@example.com'], [2, 'bob@example.com'], [3, 'john@example.com']]\n",
    "person = pd.DataFrame(data, columns=['id', 'email']).astype({'id': 'int64', 'email': 'object'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:25:20.779037600Z",
     "start_time": "2023-11-12T02:25:20.274408800Z"
    }
   },
   "id": "41390bfc83e5ad72"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/11 19:58:07 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/11 19:58:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/11 19:58:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "|id |email           |\n",
      "+---+----------------+\n",
      "|1  |john@example.com|\n",
      "|2  |bob@example.com |\n",
      "|3  |john@example.com|\n",
      "+---+----------------+\n"
     ]
    }
   ],
   "source": [
    "# to spark dataframe\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "person_df = spark.createDataFrame(person)\n",
    "person_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:25:32.881444400Z",
     "start_time": "2023-11-12T02:25:20.748116200Z"
    }
   },
   "id": "f5cf8892d1aeff0c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================>                                       (5 + 11) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  2| bob@example.com|\n",
      "|  1|john@example.com|\n",
      "+---+----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# solving in spark dataframe\n",
    "\n",
    "# spark does not support direct delete being OLAP system.\n",
    "# One solution is to re-write the data after applying filter. (not done in this example)\n",
    "\n",
    "person_df \\\n",
    "    .withColumn('rn', F.row_number().over(Window.partitionBy('email').orderBy('id'))) \\\n",
    "    .filter(F.col('rn') == 1) \\\n",
    "    .drop('rn') \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:25:34.574955400Z",
     "start_time": "2023-11-12T02:25:32.876667200Z"
    }
   },
   "id": "ec9a3c82e896fc46"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  2| bob@example.com|\n",
      "|  1|john@example.com|\n",
      "+---+----------------+\n"
     ]
    }
   ],
   "source": [
    "# solving in spark SQL\n",
    "\n",
    "person_df.createOrReplaceTempView('person')\n",
    "\n",
    "spark.sql('''\n",
    "select \n",
    "    id, email \n",
    "from \n",
    "    (select *,row_number() over (partition by email order by id) as rn from person) p \n",
    "where p.rn=1;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:25:35.474224200Z",
     "start_time": "2023-11-12T02:25:34.564012100Z"
    }
   },
   "id": "57c9e984c5b388e5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndelete\\nfrom person \\nwhere id in (\\n    select \\n        id \\n    from \\n        (select id,row_number() over (partition by email order by id) as rn from person) p \\n    where p.rn>1\\n );\\n'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mysql solution for OLTP:\n",
    "'''\n",
    "delete\n",
    "from person \n",
    "where id in (\n",
    "    select \n",
    "        id \n",
    "    from \n",
    "        (select id,row_number() over (partition by email order by id) as rn from person) p \n",
    "    where p.rn>1\n",
    " );\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:25:35.475230500Z",
     "start_time": "2023-11-12T02:25:35.455117700Z"
    }
   },
   "id": "660c718a1a57e94a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T02:25:36.367301100Z",
     "start_time": "2023-11-12T02:25:35.462581600Z"
    }
   },
   "id": "a77392120333a03c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
