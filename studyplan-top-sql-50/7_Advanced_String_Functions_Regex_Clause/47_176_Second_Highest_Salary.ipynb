{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [176. Second Highest Salary](https://leetcode.com/problems/second-highest-salary/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be3d7f35391b9a28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Employee\n",
    "\n",
    "<pre>+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| id          | int  |\n",
    "| salary      | int  |\n",
    "+-------------+------+</pre>\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains information about the salary of an employee.\n",
    " \n",
    "\n",
    "Write a solution to find the second highest salary from the Employee table. If there is no second highest salary, return null (return None in Pandas).\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "<pre>+----+--------+\n",
    "| id | salary |\n",
    "+----+--------+\n",
    "| 1  | 100    |\n",
    "| 2  | 200    |\n",
    "| 3  | 300    |\n",
    "+----+--------+</pre>\n",
    "Output: \n",
    "<pre>+---------------------+\n",
    "| SecondHighestSalary |\n",
    "+---------------------+\n",
    "| 200                 |\n",
    "+---------------------+</pre>\n",
    "Example 2:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "<pre>+----+--------+\n",
    "| id | salary |\n",
    "+----+--------+\n",
    "| 1  | 100    |\n",
    "+----+--------+</pre>\n",
    "Output: \n",
    "<pre>+---------------------+\n",
    "| SecondHighestSalary |\n",
    "+---------------------+\n",
    "| null                |\n",
    "+---------------------+</pre>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0ab671e7b21b87"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data = [[1, 100], [2, 200], [3, 300]] # example 1\n",
    "data = [[1, 100]]  # example 2\n",
    "employee = pd.DataFrame(data, columns=['id', 'salary']).astype({'id': 'int64', 'salary': 'int64'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T03:21:48.361358300Z",
     "start_time": "2023-11-12T03:21:48.067003900Z"
    }
   },
   "id": "c690fe03fe836cf0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/11 20:57:31 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/11 20:57:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/11 20:57:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|id |salary|\n",
      "+---+------+\n",
      "|1  |100   |\n",
      "+---+------+\n"
     ]
    }
   ],
   "source": [
    "# to spark dataframe\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "employee_df = spark.createDataFrame(employee)\n",
    "employee_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T03:21:57.814209400Z",
     "start_time": "2023-11-12T03:21:48.361358300Z"
    }
   },
   "id": "a0c69b3adb2efe9b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 20:57:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|SecondHighestSalary|\n",
      "+-------------------+\n",
      "|               null|\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, Row\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# solving in spark dataframe\n",
    "employee_df.select(F.col('salary')).distinct() \\\n",
    "    .unionAll(spark.createDataFrame([Row(col1=None)], schema=StructType([StructField(\"salary\", StringType(), True)]))) \\\n",
    "    .withColumn('rn', F.row_number().over(Window.orderBy(F.col('salary').desc()))) \\\n",
    "    .filter(F.col('rn')==2)\\\n",
    "    .select(F.col('salary').alias('SecondHighestSalary'))\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T03:21:59.682180Z",
     "start_time": "2023-11-12T03:21:57.811070800Z"
    }
   },
   "id": "462ca3b9efdf2136"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 20:57:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 20:57:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|SecondHighestSalary|\n",
      "+-------------------+\n",
      "|               null|\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# solving in spark SQL\n",
    "\n",
    "employee_df.createOrReplaceTempView('employee')\n",
    "\n",
    "spark.sql('''\n",
    "select salary as SecondHighestSalary\n",
    "from (\n",
    "    select salary, row_number() over (order by salary desc) as rn from \n",
    "    (select distinct salary from employee union all select null) n\n",
    ") s where s.rn =2;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T03:22:00.495919700Z",
     "start_time": "2023-11-12T03:21:59.672407300Z"
    }
   },
   "id": "b1ff0780d5ada9f5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T03:22:01.416670300Z",
     "start_time": "2023-11-12T03:22:00.544487800Z"
    }
   },
   "id": "51e86dd6fea67ec7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
