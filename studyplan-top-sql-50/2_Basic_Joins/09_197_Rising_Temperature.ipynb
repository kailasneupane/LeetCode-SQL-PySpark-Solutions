{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [197. Rising Temperature](https://leetcode.com/problems/rising-temperature/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c70c22238532b4fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Weather\n",
    "\n",
    "<pre>+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| recordDate    | date    |\n",
    "| temperature   | int     |\n",
    "+---------------+---------+</pre>\n",
    "id is the column with unique values for this table.\n",
    "This table contains information about the temperature on a certain day.\n",
    " \n",
    "\n",
    "Write a solution to find all dates' Id with higher temperatures compared to its previous dates (yesterday).\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Weather table:\n",
    "<pre>+----+------------+-------------+\n",
    "| id | recordDate | temperature |\n",
    "+----+------------+-------------+\n",
    "| 1  | 2015-01-01 | 10          |\n",
    "| 2  | 2015-01-02 | 25          |\n",
    "| 3  | 2015-01-03 | 20          |\n",
    "| 4  | 2015-01-04 | 30          |\n",
    "+----+------------+-------------+</pre>\n",
    "Output: \n",
    "<pre>+----+\n",
    "| id |\n",
    "+----+\n",
    "| 2  |\n",
    "| 4  |\n",
    "+----+</pre>\n",
    "Explanation: \n",
    "In 2015-01-02, the temperature was higher than the previous day (10 -> 25).\n",
    "In 2015-01-04, the temperature was higher than the previous day (20 -> 30)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d2bb4043c35056"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#Pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1, '2015-01-01', 10], [2, '2015-01-02', 25], [3, '2015-01-03', 20], [4, '2015-01-04', 30]]\n",
    "weather = pd.DataFrame(data, columns=['id', 'recordDate', 'temperature']).astype({'id':'Int64', 'recordDate':'datetime64[ns]', 'temperature':'Int64'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:57:39.603041400Z",
     "start_time": "2023-11-05T18:57:39.096655100Z"
    }
   },
   "id": "ad8b263e5f0997f3"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/05 12:03:03 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/05 12:03:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 12:03:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+\n",
      "|id |recordDate         |temperature|\n",
      "+---+-------------------+-----------+\n",
      "|1  |2015-01-01 00:00:00|10         |\n",
      "|2  |2015-01-02 00:00:00|25         |\n",
      "|3  |2015-01-03 00:00:00|20         |\n",
      "|4  |2015-01-04 00:00:00|30         |\n",
      "+---+-------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# to pyspark schema\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "weather_df = spark.createDataFrame(weather)\n",
    "weather_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:57:56.580409200Z",
     "start_time": "2023-11-05T18:57:39.614455200Z"
    }
   },
   "id": "4e00eb4aa7c62213"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 12:03:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# in Spark Dataframe\n",
    "\n",
    "weather_df\\\n",
    "    .withColumn('l',F.lag('temperature').over(Window.orderBy('id')))\\\n",
    "    .filter(F.col('temperature')>F.col('l'))\\\n",
    "    .select('id')\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:57:58.602713600Z",
     "start_time": "2023-11-05T18:57:56.561424800Z"
    }
   },
   "id": "9c647b45dc8ee9da"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 12:03:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/05 12:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n"
     ]
    }
   ],
   "source": [
    "# in Spark SQL\n",
    "weather_df.createOrReplaceTempView('weather')\n",
    "spark.sql('''\n",
    "select id from (\n",
    "    select \n",
    "        id, recorddate, temperature,\n",
    "        lag(temperature) over (order by id) as l \n",
    "    from weather\n",
    ") w\n",
    "where w.l<w.temperature\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:57:59.755259900Z",
     "start_time": "2023-11-05T18:57:58.613633300Z"
    }
   },
   "id": "fbe69d8b8cc3845"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:58:00.654553200Z",
     "start_time": "2023-11-05T18:57:59.734919200Z"
    }
   },
   "id": "3b09d0d3e7837236"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:58:00.679245Z",
     "start_time": "2023-11-05T18:58:00.616922200Z"
    }
   },
   "id": "b0649e0a837de07d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
