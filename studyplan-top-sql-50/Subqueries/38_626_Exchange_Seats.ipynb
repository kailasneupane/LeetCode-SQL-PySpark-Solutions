{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [626. Exchange Seats](https://leetcode.com/problems/exchange-seats/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f533c4afe16b9f93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Seat\n",
    "\n",
    "<pre>+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| student     | varchar |\n",
    "+-------------+---------+</pre>\n",
    "id is the primary key (unique value) column for this table.\n",
    "Each row of this table indicates the name and the ID of a student.\n",
    "id is a continuous increment.\n",
    " \n",
    "\n",
    "Write a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.\n",
    "\n",
    "Return the result table ordered by id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Seat table:\n",
    "<pre>+----+---------+\n",
    "| id | student |\n",
    "+----+---------+\n",
    "| 1  | Abbot   |\n",
    "| 2  | Doris   |\n",
    "| 3  | Emerson |\n",
    "| 4  | Green   |\n",
    "| 5  | Jeames  |\n",
    "+----+---------+</pre>\n",
    "Output: \n",
    "<pre>+----+---------+\n",
    "| id | student |\n",
    "+----+---------+\n",
    "| 1  | Doris   |\n",
    "| 2  | Abbot   |\n",
    "| 3  | Green   |\n",
    "| 4  | Emerson |\n",
    "| 5  | Jeames  |\n",
    "+----+---------+</pre>\n",
    "Explanation: \n",
    "Note that if the number of students is odd, there is no need to change the last one's seat."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed9023490c2396cc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1, 'Abbot'], [2, 'Doris'], [3, 'Emerson'], [4, 'Green'], [5, 'Jeames']]\n",
    "seat = pd.DataFrame(data, columns=['id', 'student']).astype({'id': 'Int64', 'student': 'object'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T22:38:03.935248200Z",
     "start_time": "2023-11-05T22:38:03.500927700Z"
    }
   },
   "id": "26d2a2408ac50066"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/05 15:32:02 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/05 15:32:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 15:32:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Abbot|\n",
      "|  2|  Doris|\n",
      "|  3|Emerson|\n",
      "|  4|  Green|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n"
     ]
    }
   ],
   "source": [
    "#to spark dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "seat_df = spark.createDataFrame(seat)\n",
    "seat_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T22:38:13.805340900Z",
     "start_time": "2023-11-05T22:38:03.925530800Z"
    }
   },
   "id": "bcf9127dc7f50abc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n"
     ]
    }
   ],
   "source": [
    "max_id = seat_df.agg(F.max('id')).collect()[0][0]\n",
    "\n",
    "seat_df.withColumn('id',\n",
    "                   F.when(F.col('id') % 2 == 0, F.col('id') - 1)\n",
    "                   .when(F.col('id') == max_id, F.col('id'))\n",
    "                   .otherwise(F.col('id') + 1)) \\\n",
    "    .orderBy('id') \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T22:38:15.529930300Z",
     "start_time": "2023-11-05T22:38:13.795704900Z"
    }
   },
   "id": "3deb928250c7f530"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n"
     ]
    }
   ],
   "source": [
    "# In spark SQL\n",
    "\n",
    "seat_df.createOrReplaceTempView('seat')\n",
    "\n",
    "spark.sql('''\n",
    "with max_id as (\n",
    "    select max(id) as max_id from seat\n",
    ")\n",
    "select \n",
    "    case \n",
    "        when id%2=0 then id-1\n",
    "        when id=(select max_id from max_id) then id\n",
    "        else id+1 end as id,\n",
    "    student\n",
    "from seat\n",
    "order by id;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T22:38:16.710598100Z",
     "start_time": "2023-11-05T22:38:15.524492700Z"
    }
   },
   "id": "c2b0a9a5a01b9d6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using subquery in spark sql\n",
    "\n",
    "spark.sql('''\n",
    "select \n",
    "    case \n",
    "        when id%2=0 then id-1\n",
    "        when id=(select max(id) from seat) then id\n",
    "        else id+1 end as id,\n",
    "    student\n",
    "from seat\n",
    "order by id;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-05T22:38:16.703260Z"
    }
   },
   "id": "65eec110c24c1d6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cec006414007f233"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ad78b3d5173a0ae8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
