{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [1321. Restaurant Growth](https://leetcode.com/problems/restaurant-growth/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8febaf4d87891bed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Customer\n",
    "\n",
    "<pre>+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| name          | varchar |\n",
    "| visited_on    | date    |\n",
    "| amount        | int     |\n",
    "+---------------+---------+</pre>\n",
    "In SQL,(customer_id, visited_on) is the primary key for this table.\n",
    "This table contains data about customer transactions in a restaurant.\n",
    "visited_on is the date on which the customer with ID (customer_id) has visited the restaurant.\n",
    "amount is the total paid by a customer.\n",
    " \n",
    "\n",
    "You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day).\n",
    "\n",
    "Compute the moving average of how much the customer paid in a seven days window (i.e., current day + 6 days before). average_amount should be rounded to two decimal places.\n",
    "\n",
    "Return the result table ordered by visited_on in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customer table:\n",
    "<pre>+-------------+--------------+--------------+-------------+\n",
    "| customer_id | name         | visited_on   | amount      |\n",
    "+-------------+--------------+--------------+-------------+\n",
    "| 1           | Jhon         | 2019-01-01   | 100         |\n",
    "| 2           | Daniel       | 2019-01-02   | 110         |\n",
    "| 3           | Jade         | 2019-01-03   | 120         |\n",
    "| 4           | Khaled       | 2019-01-04   | 130         |\n",
    "| 5           | Winston      | 2019-01-05   | 110         | \n",
    "| 6           | Elvis        | 2019-01-06   | 140         | \n",
    "| 7           | Anna         | 2019-01-07   | 150         |\n",
    "| 8           | Maria        | 2019-01-08   | 80          |\n",
    "| 9           | Jaze         | 2019-01-09   | 110         | \n",
    "| 1           | Jhon         | 2019-01-10   | 130         | \n",
    "| 3           | Jade         | 2019-01-10   | 150         | \n",
    "+-------------+--------------+--------------+-------------+</pre>\n",
    "Output: \n",
    "<pre>+--------------+--------------+----------------+\n",
    "| visited_on   | amount       | average_amount |\n",
    "+--------------+--------------+----------------+\n",
    "| 2019-01-07   | 860          | 122.86         |\n",
    "| 2019-01-08   | 840          | 120            |\n",
    "| 2019-01-09   | 840          | 120            |\n",
    "| 2019-01-10   | 1000         | 142.86         |\n",
    "+--------------+--------------+----------------+</pre>\n",
    "Explanation: \n",
    "1st moving average from 2019-01-01 to 2019-01-07 has an average_amount of (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86\n",
    "2nd moving average from 2019-01-02 to 2019-01-08 has an average_amount of (110 + 120 + 130 + 110 + 140 + 150 + 80)/7 = 120\n",
    "3rd moving average from 2019-01-03 to 2019-01-09 has an average_amount of (120 + 130 + 110 + 140 + 150 + 80 + 110)/7 = 120\n",
    "4th moving average from 2019-01-04 to 2019-01-10 has an average_amount of (130 + 110 + 140 + 150 + 80 + 110 + 130 + 150)/7 = 142.86"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93601caf4721e786"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1, 'Jhon', '2019-01-01', 100], [2, 'Daniel', '2019-01-02', 110], [3, 'Jade', '2019-01-03', 120],\n",
    "        [4, 'Khaled', '2019-01-04', 130], [5, 'Winston', '2019-01-05', 110], [6, 'Elvis', '2019-01-06', 140],\n",
    "        [7, 'Anna', '2019-01-07', 150], [8, 'Maria', '2019-01-08', 80], [9, 'Jaze', '2019-01-09', 110],\n",
    "        [1, 'Jhon', '2019-01-10', 130], [3, 'Jade', '2019-01-10', 150]]\n",
    "       # [1, 'Jhon', '2019-01-10', 130], [3, 'Jade', '2019-01-10', 150],[3, 'Jade', '2019-01-12', 150]]\n",
    "customer = pd.DataFrame(data, columns=['customer_id', 'name', 'visited_on', 'amount']).astype(\n",
    "    {'customer_id': 'Int64', 'name': 'object', 'visited_on': 'datetime64[ns]', 'amount': 'Int64'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:07.309988600Z",
     "start_time": "2023-11-07T03:35:06.353303900Z"
    }
   },
   "id": "f6a776317d65bcf4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/06 15:43:35 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/06 15:43:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/06 15:43:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------------+------+\n",
      "|customer_id|   name|         visited_on|amount|\n",
      "+-----------+-------+-------------------+------+\n",
      "|          1|   Jhon|2019-01-01 00:00:00|   100|\n",
      "|          2| Daniel|2019-01-02 00:00:00|   110|\n",
      "|          3|   Jade|2019-01-03 00:00:00|   120|\n",
      "|          4| Khaled|2019-01-04 00:00:00|   130|\n",
      "|          5|Winston|2019-01-05 00:00:00|   110|\n",
      "|          6|  Elvis|2019-01-06 00:00:00|   140|\n",
      "|          7|   Anna|2019-01-07 00:00:00|   150|\n",
      "|          8|  Maria|2019-01-08 00:00:00|    80|\n",
      "|          9|   Jaze|2019-01-09 00:00:00|   110|\n",
      "|          1|   Jhon|2019-01-10 00:00:00|   130|\n",
      "|          3|   Jade|2019-01-10 00:00:00|   150|\n",
      "+-----------+-------+-------------------+------+\n"
     ]
    }
   ],
   "source": [
    "#to spark dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "customer_df = spark.createDataFrame(customer)\n",
    "customer_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:26.207968800Z",
     "start_time": "2023-11-07T03:35:07.309988600Z"
    }
   },
   "id": "edc06e7794086dc9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    min(visited_on)|    max(visited_on)|\n",
      "+-------------------+-------------------+\n",
      "|2019-01-01 00:00:00|2019-01-10 00:00:00|\n",
      "+-------------------+-------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "# Solving in Spark DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "min_max_dates = customer_df.agg(F.min('visited_on'),F.max('visited_on'))\n",
    "min_max_dates.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:28.009034500Z",
     "start_time": "2023-11-07T03:35:26.207705500Z"
    }
   },
   "id": "77315c827e8f8044"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|         visited_on|amount|\n",
      "+-------------------+------+\n",
      "|2019-01-01 00:00:00|     0|\n",
      "|2019-01-02 00:00:00|     0|\n",
      "|2019-01-03 00:00:00|     0|\n",
      "|2019-01-04 00:00:00|     0|\n",
      "|2019-01-05 00:00:00|     0|\n",
      "|2019-01-06 00:00:00|     0|\n",
      "|2019-01-07 00:00:00|     0|\n",
      "|2019-01-08 00:00:00|     0|\n",
      "|2019-01-09 00:00:00|     0|\n",
      "|2019-01-10 00:00:00|     0|\n",
      "+-------------------+------+\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "min_date = min_max_dates.collect()[0][0]\n",
    "max_date = min_max_dates.collect()[0][1]\n",
    "\n",
    "days_diff = max_date-min_date\n",
    "from_min_to_max_date_list = [(min_date + timedelta(days=i),0) for i in range(0,days_diff.days+1)]\n",
    "\n",
    "dummy_df_header = ['visited_on','amount']\n",
    "dummy_df = spark.createDataFrame(from_min_to_max_date_list,dummy_df_header)\n",
    "dummy_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:30.022327400Z",
     "start_time": "2023-11-07T03:35:27.991835600Z"
    }
   },
   "id": "2cb529bd317a94ce"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 15:43:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:43:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:43:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:43:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:43:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:43:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:43:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------------+\n",
      "|         visited_on|amount|average_amount|\n",
      "+-------------------+------+--------------+\n",
      "|2019-01-07 00:00:00|   860|        122.86|\n",
      "|2019-01-08 00:00:00|   840|         120.0|\n",
      "|2019-01-09 00:00:00|   840|         120.0|\n",
      "|2019-01-10 00:00:00|  1000|        142.86|\n",
      "+-------------------+------+--------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = customer_df \\\n",
    "    .select('visited_on','amount')\\\n",
    "    .unionAll(dummy_df)\\\n",
    "    .groupBy('visited_on') \\\n",
    "    .agg(F.sum('amount').alias('amount')) \\\n",
    "    .withColumn('amount', F.sum('amount').over(Window.orderBy('visited_on').rowsBetween(-6, 0)))\\\n",
    "    .withColumn('average_amount', F.round(F.col('amount')/7,2))\\\n",
    "    .withColumn('rn', F.row_number().over(Window.orderBy('visited_on')))\\\n",
    "    .filter(F.col('rn')>=7)\\\n",
    "    .select('visited_on','amount','average_amount')\\\n",
    "    .orderBy('visited_on')\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:33.118124900Z",
     "start_time": "2023-11-07T03:35:30.011538200Z"
    }
   },
   "id": "d7ecd55bc8a1e48"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 15:44:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:44:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:44:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:44:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:44:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:44:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 15:44:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------------+\n",
      "|         visited_on|amount|average_amount|\n",
      "+-------------------+------+--------------+\n",
      "|2019-01-07 00:00:00|   860|        122.86|\n",
      "|2019-01-08 00:00:00|   840|         120.0|\n",
      "|2019-01-09 00:00:00|   840|         120.0|\n",
      "|2019-01-10 00:00:00|  1000|        142.86|\n",
      "+-------------------+------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Solving in spark SQL\n",
    "\n",
    "customer_df.createOrReplaceTempView('customer')\n",
    "# this solution will not work if there is missing visited_on between start and end date.\n",
    "spark.sql('''\n",
    "select visited_on, amount, average_amount from\n",
    "(select \n",
    "    visited_on, \n",
    "    sum(amount) over (order by visited_on rows between 6 preceding and current row) as amount,\n",
    "    round(avg(amount) over (order by visited_on rows between 6 preceding and current row),2) as average_amount,\n",
    "    row_number() over (order by visited_on) as rn\n",
    "from (\n",
    "    select visited_on, sum(amount) as amount\n",
    "    from customer\n",
    "    group by visited_on\n",
    ")) where rn>=7;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:34.542073100Z",
     "start_time": "2023-11-07T03:35:33.138580Z"
    }
   },
   "id": "5e7692dc57fe287a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n WITH RECURSIVE date_range AS (\\n  SELECT (select min(visited_on) from customer) AS visited_on, 0 as amount\\n  UNION ALL\\n  SELECT DATE_ADD(visited_on, INTERVAL 1 DAY), 0 as amount\\n  FROM date_range\\n  WHERE DATE_ADD(visited_on, INTERVAL 1 DAY) <= (select max(visited_on) from customer)\\n), temp_table as (\\n    select * from date_range\\n    union all\\n    select visited_on, amount as amount from customer\\n), tbl as (\\n    select \\n        visited_on, \\n        sum(amount) over (order by visited_on rows between 6 preceding and current row) as amount,\\n        round(avg(amount) over (order by visited_on rows between 6 preceding and current row),2) as average_amount,\\n        row_number() over (order by visited_on) as rn\\n    from (\\n        select visited_on, sum(amount) as amount\\n        from temp_table\\n        group by visited_on\\n    ) t\\n)\\nselect visited_on, amount, average_amount\\nfrom tbl\\nwhere rn >=7;\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this mysql solution will work for the data with missing dates as well\n",
    "\n",
    "'''\n",
    " WITH RECURSIVE date_range AS (\n",
    "  SELECT (select min(visited_on) from customer) AS visited_on, 0 as amount\n",
    "  UNION ALL\n",
    "  SELECT DATE_ADD(visited_on, INTERVAL 1 DAY), 0 as amount\n",
    "  FROM date_range\n",
    "  WHERE DATE_ADD(visited_on, INTERVAL 1 DAY) <= (select max(visited_on) from customer)\n",
    "), temp_table as (\n",
    "    select * from date_range\n",
    "    union all\n",
    "    select visited_on, amount as amount from customer\n",
    "), tbl as (\n",
    "    select \n",
    "        visited_on, \n",
    "        sum(amount) over (order by visited_on rows between 6 preceding and current row) as amount,\n",
    "        round(avg(amount) over (order by visited_on rows between 6 preceding and current row),2) as average_amount,\n",
    "        row_number() over (order by visited_on) as rn\n",
    "    from (\n",
    "        select visited_on, sum(amount) as amount\n",
    "        from temp_table\n",
    "        group by visited_on\n",
    "    ) t\n",
    ")\n",
    "select visited_on, amount, average_amount\n",
    "from tbl\n",
    "where rn >=7;\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T03:35:34.758328300Z",
     "start_time": "2023-11-07T03:35:34.516094800Z"
    }
   },
   "id": "8fe54eefdd4d7b46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
