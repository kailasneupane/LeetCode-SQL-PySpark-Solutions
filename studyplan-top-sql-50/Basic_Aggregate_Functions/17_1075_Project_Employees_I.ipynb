{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [1075. Project Employees I](https://leetcode.com/problems/project-employees-i/description/?envType=study-plan-v2&envId=top-sql-50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f9ea72ca5991a99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table: Project\n",
    "\n",
    "<pre>+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| project_id  | int     |\n",
    "| employee_id | int     |\n",
    "+-------------+---------+</pre>\n",
    "(project_id, employee_id) is the primary key of this table.\n",
    "employee_id is a foreign key to Employee table.\n",
    "Each row of this table indicates that the employee with employee_id is working on the project with project_id.\n",
    " \n",
    "\n",
    "Table: Employee\n",
    "\n",
    "<pre>+------------------+---------+\n",
    "| Column Name      | Type    |\n",
    "+------------------+---------+\n",
    "| employee_id      | int     |\n",
    "| name             | varchar |\n",
    "| experience_years | int     |\n",
    "+------------------+---------+</pre>\n",
    "employee_id is the primary key of this table. It's guaranteed that experience_years is not NULL.\n",
    "Each row of this table contains information about one employee.\n",
    " \n",
    "\n",
    "Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Project table:\n",
    "<pre>+-------------+-------------+\n",
    "| project_id  | employee_id |\n",
    "+-------------+-------------+\n",
    "| 1           | 1           |\n",
    "| 1           | 2           |\n",
    "| 1           | 3           |\n",
    "| 2           | 1           |\n",
    "| 2           | 4           |\n",
    "+-------------+-------------+</pre>\n",
    "Employee table:\n",
    "<pre>+-------------+--------+------------------+\n",
    "| employee_id | name   | experience_years |\n",
    "+-------------+--------+------------------+\n",
    "| 1           | Khaled | 3                |\n",
    "| 2           | Ali    | 2                |\n",
    "| 3           | John   | 1                |\n",
    "| 4           | Doe    | 2                |\n",
    "+-------------+--------+------------------+</pre>\n",
    "Output: \n",
    "<pre>+-------------+---------------+\n",
    "| project_id  | average_years |\n",
    "+-------------+---------------+\n",
    "| 1           | 2.00          |\n",
    "| 2           | 2.50          |\n",
    "+-------------+---------------+</pre>\n",
    "Explanation: The average experience years for the first project is (3 + 2 + 1) / 3 = 2.00 and for the second project is (3 + 2) / 2 = 2.50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cd79b44bf4f8805"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[1, 1], [1, 2], [1, 3], [2, 1], [2, 4]]\n",
    "project = pd.DataFrame(data, columns=['project_id', 'employee_id']).astype(\n",
    "    {'project_id': 'Int64', 'employee_id': 'Int64'})\n",
    "data = [[1, 'Khaled', 3], [2, 'Ali', 2], [3, 'John', 1], [4, 'Doe', 2]]\n",
    "employee = pd.DataFrame(data, columns=['employee_id', 'name', 'experience_years']).astype(\n",
    "    {'employee_id': 'Int64', 'name': 'object', 'experience_years': 'Int64'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:58:36.589959100Z",
     "start_time": "2023-11-07T18:58:35.905764200Z"
    }
   },
   "id": "5dedfbf50a9e19dd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "23/11/07 12:58:39 WARN Utils: Your hostname, svchost resolves to a loopback address: 127.0.1.1; using 172.18.28.34 instead (on interface eth0)\n",
      "23/11/07 12:58:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/07 12:58:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|1         |1          |\n",
      "|1         |2          |\n",
      "|1         |3          |\n",
      "|2         |1          |\n",
      "|2         |4          |\n",
      "+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# to pyspark dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "project_df = spark.createDataFrame(project)\n",
    "project_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:58:53.133868900Z",
     "start_time": "2023-11-07T18:58:36.596802800Z"
    }
   },
   "id": "d8153d6757bf3d8c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------------+\n",
      "|employee_id|name  |experience_years|\n",
      "+-----------+------+----------------+\n",
      "|1          |Khaled|3               |\n",
      "|2          |Ali   |2               |\n",
      "|3          |John  |1               |\n",
      "|4          |Doe   |2               |\n",
      "+-----------+------+----------------+\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.createDataFrame(employee)\n",
    "employee_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:58:54.182852800Z",
     "start_time": "2023-11-07T18:58:53.130749400Z"
    }
   },
   "id": "ba17991e49eb7f99"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|project_id|average_years|\n",
      "+----------+-------------+\n",
      "|         1|          2.0|\n",
      "|         2|          2.5|\n",
      "+----------+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# solving in spark dataframe\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "project_df\\\n",
    "    .join(employee_df,on=['employee_id'],how='inner')\\\n",
    "    .groupBy('project_id').agg(F.avg('experience_years').alias('average_years'))\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:58:57.289471900Z",
     "start_time": "2023-11-07T18:58:54.179789200Z"
    }
   },
   "id": "cdccdbd6911be52e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=======>                                                (2 + 14) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|project_id|average_years|\n",
      "+----------+-------------+\n",
      "|         1|          2.0|\n",
      "|         2|          2.5|\n",
      "+----------+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# solving in spark sql\n",
    "\n",
    "employee_df.createOrReplaceTempView('employee')\n",
    "project_df.createOrReplaceTempView('project')\n",
    "\n",
    "spark.sql('''\n",
    "select project_id, round(avg(experience_years),2) as average_years\n",
    "from project\n",
    "inner join employee\n",
    "on project.employee_id=employee.employee_id\n",
    "group by project_id;\n",
    "''').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:58:58.996932900Z",
     "start_time": "2023-11-07T18:58:57.301425Z"
    }
   },
   "id": "8e50ae56b0a60e3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:59:09.040790Z",
     "start_time": "2023-11-07T18:59:08.025210300Z"
    }
   },
   "id": "c63aa4e20e857a0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b1e8d346096992e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
